{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using lakeFS-spec to interact with lakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### lakeFS-spec makes versioned data available via a filesystem interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing `lakeFS-spec` you can use file-system identifiers (`lakefs://`) to reference URIs to the lakeFS storage. The installation (`pip install lakefs-spec`) registers the identifier with fsspec.\n",
    "That way, without even importing the lakefs-spec, all libraries that use fsspec under the hood, like pandas or DuckDb, can work with filepaths such as in the next cell to fetch data from remote storage locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"lakefs://pydata-hn/main/lakes.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The same way, we can write files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write files in the same way. However, to ensure a clean state of the remote lakeFS repository even if errors occur in the versioning operations we recommend to use the transaction context manager to conduct versioning operations.\n",
    "\n",
    "To start a transaction, we have to first instantiate a `LakeFSFileSystem` object. From the instance we use the `.transaction()` method to launch a context manager.\n",
    "Under the hood the context manager collects the versioning operations in placeholder actions. Once you exit the transaction, the operations are sent off to lakeFS in a batch. That way, we catch possible errors before sending instructions to lakeFS and avoid a repository in a dangling state. \n",
    "\n",
    "Additionally, each transaction creates a temporary branch (that is persisted if you want) on which the operations are performed. This safety guard prevents corrupting the repository state, should errors happen during the execution of the versioning operations on the remote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "german_lakes = df[df['Country'] == \"Germany\"]\n",
    "german_lakes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from lakefs_spec import LakeFSFileSystem\n",
    "\n",
    "fs = LakeFSFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with fs.transaction(\"pydata-hn\", \"main\") as tx:\n",
    "    german_lakes.to_parquet(f\"lakefs://{tx.repository}/{tx.branch.id}/german_lakes.parquet\")\n",
    "    tx.commit(message=\"Extract German lakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can access arbitrary files with `open()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access arbitrary files and not be reliant on an fsspec implementation in a library, we can use Pythons builtin `open()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with fs.transaction(\"pydata-hn\", \"main\") as tx:\n",
    "    with fs.open(f\"lakefs://{tx.repository}/{tx.branch.id}/experiment.json\", \"w\") as f:\n",
    "        data = Path(\"experiment.json\").read_text()\n",
    "        json.dump(data, f)\n",
    "    tx.commit(message=\"Add experiment json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### With the transaction API, we can perform complex versioning operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transaction API supports complex versioning operations available in lakeFS and with which you may be familiar with from tools like git.\n",
    "Namely, on top of reading and writing files, within a `fs.transaction` you can `.commit`, `.revert`, `.merge`, `.tag` a repository state reference (e.g. a commit or branch), and `.rev_parse` to parse reference trees.\n",
    "\n",
    "You can also access the current `.branch`, `.base_branch`, `.repository`, and the files on the branch with `.files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with fs.transaction(\n",
    "    \"pydata-hn\",\n",
    "    base_branch=\"main\",\n",
    "    branch_name=\"demo-experiment\",\n",
    "    automerge=False,\n",
    "    delete=\"never\",\n",
    ") as tx:\n",
    "    train.to_csv(f\"lakefs://{tx.repository}/{tx.branch.id}/train.csv\")\n",
    "    test.to_csv(f\"lakefs://{tx.repository}/{tx.branch.id}/test.csv\")\n",
    "    \n",
    "    commit = tx.commit(message=\"Create train test split\")\n",
    "print(commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can also merge branches and reference repository states using tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the `commit` object holds a unique SHA that identifies the specific data repository state. We can use tags to provide human readable references. For automated versioning as well as experiment tracking we can use the unique identifiers.\n",
    "\n",
    "Tags are immutable so that you do not accidentally break any code existing elsewhere would you reassign a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with fs.transaction(\"pydata-hn\", \"main\"):\n",
    "    tx.merge(source_ref=\"main\", into=\"demo-experiment\")\n",
    "    tag = tx.tag(ref=commit.id, name=\"PyDataDemo\")\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"lakefs://pydata-hn/PyDataDemo/test.csv\", index_col=0)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can use unique identifiers for automated versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f\"lakefs://pydata-hn/{commit.id}/lakes.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "lakeFS & lakeFS-spec\n",
    "- Easy read an write operations by adding lakeFS URIs to your filesystem\n",
    "- Git-style versioning and collaboration features\n",
    "- Transactions as a safeguarded way to programmatically conduct versioning operations\n",
    "\n",
    "Niceties\n",
    "- Automatic authentication discovery\n",
    "- Caching for up and downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can visit the lakeFS-spec [GitHub Repository](https://github.com/aai-institute/lakefs-spec) and our [documentation](https://lakefs-spec.org/latest/).\n",
    "You can start using lakeFS-spec now with:\n",
    "`pip install lakefs-spec`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
